{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Framework\n",
    "\n",
    "- ### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architetury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"files/image.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module):\n",
    "\n",
    "    # Construtor \n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, dropout):\n",
    "\n",
    "        #inicializa o construtor da classe m√£e (nn.Module)\n",
    "        super().__init__()\n",
    "\n",
    "        # Atributos\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads \n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        #camada de embedding - sequ√™ncia de entrada para senquencia de vetores\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #mecanismo de auto-aten√ß√£o multi-head\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, n_heads, dropout = dropout)\n",
    "        \n",
    "        #rede neural feed-forward - gwera seq de sa√≠da a partir da entrada\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        #camda de sa√≠da\n",
    "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "def forward(self, x):\n",
    "    \n",
    "    #entrada\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    #multi-head\n",
    "    x = self.attention(x)\n",
    "    \n",
    "    #feed-forward \n",
    "    x = self.feed_forward(x)\n",
    "    \n",
    "    x = self.out(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Transformers(\n",
       "  (embedding): Embedding(1000, 32)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (feed_forward): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (out): Linear(in_features=32, out_features=32, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Transformers(vocab_size= 1000, \n",
    "                      embedding_dim = 32,\n",
    "                      n_heads = 4,\n",
    "                      n_layers = 3,\n",
    "                      dropout = 0.4)\n",
    "\n",
    "modelo.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Frameworks\n",
    "\n",
    "- ### Only - attetion mecanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model consists of several main parts:\n",
    "\n",
    "- 1- Embedding Layer: Transforms words into numerical vectors of fixed size.\n",
    "- 2- Attention Mechanism: Allows the model to focus on different parts of the input.\n",
    "- 3- Encoder and Decoder Layers: Process data sequentially.\n",
    "- 4- Linear and Softmax Layer: For final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 64\n",
    "seq_length = 10\n",
    "vocab_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Provide a rich and dense representation of words or tokens, capturing contextual and semantic information that is essential for tasks such as machine translation, text classification, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(input, vocab_size, dim_model):\n",
    "    # Sequ√™ncia em contexto \n",
    "    \n",
    "    #cria matriz onde cada linha representa um token do vocab - valores aleat√≥rios\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "    \n",
    "    # para cada √≠ndice de token no input, √© selecionado o embedding correspondente da matriz\n",
    "    # array de embeddings da sequ√™ncia de entrada - a sequ√™ncia est√° em contexto  \n",
    "    \n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mecanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query, Key and Value - Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"files/image copy 5.png\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/image copy 4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q (query): Query is the \"search representation\". It is generated by multiplying the input \n",
    "ùëã by a weight matriz Wq. \n",
    "\n",
    "ùëÑ = XWq\n",
    "\n",
    "Q encodes what we are trying to find elsewhere in the sequence. For example, when processing a word, ùëÑ may represent the question: \"Who should this word relate to?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K (Key):  is like a \"label\" that represents the semantic content of the input. It is generated by multiplying ùëã by a weight matrix  Wk\n",
    "\n",
    "ùêæ = ùëã Wk\n",
    "\n",
    " - ùêæ is used to determine how relevant a part of the input is to the question asked by the ùëÑ. Does it answer the question: \"Do I have the information you are looking for?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Value contains the actual information that will be used in the final result. It is generated by multiplying ùëã by a weight matrix Wv.\n",
    "\n",
    "V=XW \n",
    "\n",
    "After determining the relevant parts using ùëÑ and K, the ùëâ delivers the related \"content\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of \\( Q \\), \\( K \\), and \\( V \\) in Transformers\n",
    "\n",
    "Value (\\( V \\))\n",
    "- The **Value** represents the actual information that will be used in the final result.\n",
    "- It is derived by multiplying the input \\( X \\) by a weight matrix \\( W_V \\):  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After determining the relevant parts using \\( Q \\) (Query) and \\( K \\) (Key), the \\( V \\) provides the \"content\" related to the calculated attention.\n",
    "\n",
    "---\n",
    "\n",
    "How do \\( Q \\), \\( K \\), and \\( V \\) interact?\n",
    "\n",
    "The interaction between \\( Q \\), \\( K \\), and \\( V \\) occurs during the **attention calculation**, where the model decides which values (\\( V \\)) are most important for a given query (\\( Q \\)).\n",
    "\n",
    "1. Dot Product of \\( Q \\) and \\( K \\)\n",
    "- First, the similarity between \\( Q \\) (what we are looking for) and \\( K \\) (what is available) is calculated using the dot product:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This measures how strongly \\( Q \\) and \\( K \\) are related.\n",
    "\n",
    "2. Normalization with Softmax\n",
    "- The result of the dot product is passed through a softmax function to produce weights that sum to 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These weights indicate how much attention each part of the input should receive.  \n",
    "- The denominator \\( \\sqrt{d_k} \\) (the dimension of \\( K \\)) is a scaling factor that helps stabilize training by preventing large gradients.\n",
    "\n",
    " 3. Weighting the Values (\\( V \\))\n",
    "- The attention weights are then applied to \\( V \\), combining the values based on their relevance:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The result is a weighted sum of \\( V \\), where the weights come from the similarity between \\( Q \\) and \\( K \\).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. **\\( Q \\)**: Defines what information is being searched for (query).  \n",
    "2. **\\( K \\)**: Represents the available information (key).  \n",
    "3. **\\( V \\)**: Contains the actual data that will be used (value).  \n",
    "\n",
    "- The **attention mechanism** ensures that each word or token can \"focus\" on other parts of the input sequence that are contextually relevant. This enables the model to understand relationships in the data dynamically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    return e_x/ e_x.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dot_product_attention(Q, V, K):\n",
    "    \n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "    \n",
    "    #dimens√£o de K\n",
    "    depth = K.shape[-1]\n",
    "    \n",
    "    logits = matmul_qk/ np.sqrt(depth)\n",
    "    \n",
    "    attetion_weights= softmax(logits)\n",
    "    \n",
    "    output = np.dot(attetion_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_and_softmax(input):\n",
    "    \n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "    \n",
    "    logits = np.dot(input, weights)\n",
    "    \n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(input):\n",
    "    \n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    attention_output = scale_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "    \n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    \n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: [2 2 5 2 6 7 1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "input_sequence = np.random.randint(0, vocab_size, seq_length)\n",
    "print(f\"sequence: {input_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput: [5 5 0 5 4 0 9 5 9 9]\n"
     ]
    }
   ],
   "source": [
    "output = transformer_model(input_sequence)\n",
    "print(f\"Ouput: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
